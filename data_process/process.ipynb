{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    p_text = \" \".join(text.split(\"\\r\\n\"))\n",
    "    p_text = \" \".join(p_text.split(\"\\n\\r\"))\n",
    "    p_text = \" \".join(p_text.split(\"\\n\"))\n",
    "    p_text = \" \".join(p_text.split(\"\\t\"))\n",
    "    p_text = \" \".join(p_text.split(\"\\rm\"))\n",
    "    p_text = \" \".join(p_text.split(\"\\r\"))\n",
    "    p_text = \"\".join(p_text.split(\"$\"))\n",
    "    p_text = \"\".join(p_text.split(\"*\"))\n",
    "\n",
    "    return p_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creat dir: /scratch/user/uqyliu71/Patton/data/pubmed/nc\n",
      "Creat dir: /scratch/user/uqyliu71/Patton/data/pubmed/nc-coarse\n",
      "Creat dir: /scratch/user/uqyliu71/Patton/data/pubmed/neighbor\n",
      "Creat dir: /scratch/user/uqyliu71/Patton/data/pubmed/self-train\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"pubmed\"\n",
    "base_path = \"/scratch/user/uqyliu71/Patton/data\"\n",
    "dataset_path = os.path.join(base_path, dataset_name)\n",
    "\n",
    "subfolders = [\"nc\", \"nc-coarse\", \"neighbor\", \"self-train\"]\n",
    "\n",
    "for folder in subfolders:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Creat dir: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Dir has already existed: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19717it [00:00, 3605645.80it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_file = os.path.join(base_path, f\"{dataset_name}.pt\")\n",
    "datasets = torch.load(dataset_file)\n",
    "\n",
    "data_text = [text_process(text) for text in datasets.raw_texts]\n",
    "data = {i: text for i, text in tqdm(enumerate(data_text))}\n",
    "ref_paper = data\n",
    "label_name_set = datasets.label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19717 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [00:00<00:00, 89777.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test size:58971,5296,17799\n",
      "Train/Val/Test avg:2.990870822133185,0.26860069990363644,0.9027235380636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "test_pairs = []\n",
    "train_pair_set = set()\n",
    "item_id2idx = {}\n",
    "train_neighbor = defaultdict(list)\n",
    "val_neighbor = defaultdict(list)\n",
    "test_neighbor = defaultdict(list)\n",
    "\n",
    "edge_index = np.array(datasets.edge_index)  # 创建一个字典来保存每个节点的邻居\n",
    "neighbors = defaultdict(list)\n",
    "for start_node, end_node in zip(edge_index[0], edge_index[1]):\n",
    "    neighbors[start_node].append(end_node)\n",
    "\n",
    "# 现在可以使用 neighbors 字典来代替 ref_paper[iid]['reference']\n",
    "for iid in tqdm(ref_paper):\n",
    "    if iid not in item_id2idx:\n",
    "        item_id2idx[iid] = len(item_id2idx)\n",
    "\n",
    "    also_viewed = neighbors[iid]  # 使用从 edge_index 得到的邻居列表\n",
    "    # random.shuffle(also_viewed)\n",
    "\n",
    "    for i in range(int(len(also_viewed) * 0.8)):\n",
    "        train_pairs.append((iid, also_viewed[i]))\n",
    "        train_pair_set.add((iid, also_viewed[i]))\n",
    "        train_pair_set.add((also_viewed[i], iid))\n",
    "\n",
    "        if also_viewed[i] not in item_id2idx:\n",
    "            item_id2idx[also_viewed[i]] = len(item_id2idx)\n",
    "\n",
    "        train_neighbor[iid].append(also_viewed[i])\n",
    "\n",
    "    for i in range(int(len(also_viewed) * 0.8), int(len(also_viewed) * 0.9)):\n",
    "        if (iid, also_viewed[i]) in train_pair_set:\n",
    "            continue\n",
    "        val_pairs.append((iid, also_viewed[i]))\n",
    "        assert (iid, also_viewed[i]) not in train_pair_set\n",
    "\n",
    "        if also_viewed[i] not in item_id2idx:\n",
    "            item_id2idx[also_viewed[i]] = len(item_id2idx)\n",
    "\n",
    "        val_neighbor[iid].append(also_viewed[i])\n",
    "\n",
    "    for i in range(int(len(also_viewed) * 0.9), len(also_viewed)):\n",
    "        if (iid, also_viewed[i]) in train_pair_set:\n",
    "            continue\n",
    "        test_pairs.append((iid, also_viewed[i]))\n",
    "        assert (iid, also_viewed[i]) not in train_pair_set\n",
    "\n",
    "        if also_viewed[i] not in item_id2idx:\n",
    "            item_id2idx[also_viewed[i]] = len(item_id2idx)\n",
    "\n",
    "        test_neighbor[iid].append(also_viewed[i])\n",
    "\n",
    "print(f\"Train/Val/Test size:{len(train_pairs)},{len(val_pairs)},{len(test_pairs)}\")\n",
    "print(\n",
    "    f\"Train/Val/Test avg:{len(train_pairs)/len(ref_paper)},{len(val_pairs)/len(ref_paper)},{len(test_pairs)/len(ref_paper)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19717 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [00:22<00:00, 886.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# save all the text on node in the graph\n",
    "\n",
    "node_id_set = set()\n",
    "\n",
    "with open(os.path.join(dataset_path, \"corpus.txt\"), \"w\") as fout:\n",
    "    for iid in tqdm(ref_paper):\n",
    "        also_viewed = neighbors\n",
    "\n",
    "        # save iid text\n",
    "        if iid not in node_id_set:\n",
    "            node_id_set.add(iid)\n",
    "            fout.write(str(iid) + \"\\t\" + data[iid] + \"\\n\")\n",
    "\n",
    "        # save neighbor\n",
    "        for iid_n in also_viewed:\n",
    "            if iid_n not in node_id_set:\n",
    "                node_id_set.add(iid_n)\n",
    "                fout.write(str(iid_n) + \"\\t\" + data[iid_n] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58971/58971 [00:06<00:00, 9720.26it/s] \n"
     ]
    }
   ],
   "source": [
    "# generate and save train file\n",
    "\n",
    "random.seed(0)\n",
    "sample_neighbor_num = 16\n",
    "\n",
    "with open(os.path.join(dataset_path, \"train.text.jsonl\"), \"w\") as fout:\n",
    "    for q, k in tqdm(train_pairs):\n",
    "        q = str(q)\n",
    "        k = str(k)\n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[int(q)]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[int(k)]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        # random.shuffle(q_n_pool)\n",
    "        # random.shuffle(k_n_pool)\n",
    "\n",
    "        # # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num - len(q_n_pool))\n",
    "\n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num - len(k_n_pool))\n",
    "\n",
    "        # prepare for writing file\n",
    "        q_text = data[int(q)]\n",
    "        q_n_text = \"\\*\\*\".join([data[q_n] if q_n != -\n",
    "                               1 else \"\" for q_n in q_samples])\n",
    "        q_n_text = [data[q_n] if q_n != -1 else \"\" for q_n in q_samples]\n",
    "\n",
    "        k_text = data[int(k)]\n",
    "        # k_n_text = '\\*\\*'.join([text_process(data[k_n]) if k_n != -1 else '' for k_n in k_samples])\n",
    "        k_n_text = [data[k_n] if k_n != -1 else \"\" for k_n in k_samples]\n",
    "\n",
    "        # q_line = q_text + '\\t' + q_n_text\n",
    "        # k_line = k_text + '\\t' + k_n_text\n",
    "\n",
    "        # fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": q_text,\n",
    "                    \"q_n_text\": q_n_text,\n",
    "                    \"k_text\": k_text,\n",
    "                    \"k_n_text\": k_n_text,\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5296/5296 [00:00<00:00, 9261.46it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "with open(os.path.join(dataset_path, \"val.text.jsonl\"), \"w\") as fout:\n",
    "    for q, k in tqdm(val_pairs):\n",
    "\n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        # random.shuffle(q_n_pool)\n",
    "        # random.shuffle(k_n_pool)\n",
    "\n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num - len(q_n_pool))\n",
    "\n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num - len(k_n_pool))\n",
    "\n",
    "        # prepare for writing file\n",
    "        q_text = data[q]\n",
    "        # q_n_text = '\\*\\*'.join([text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples])\n",
    "        q_n_text = [data[q_n] if q_n != -1 else \"\" for q_n in q_samples]\n",
    "\n",
    "        k_text = data[k]\n",
    "        # k_n_text = '\\*\\*'.join([text_process(data[k_n]['title']) if k_n != -1 else '' for k_n in k_samples])\n",
    "        k_n_text = [data[k_n] if k_n != -1 else \"\" for k_n in k_samples]\n",
    "\n",
    "        # q_line = q_text + '\\t' + q_n_text\n",
    "        # k_line = k_text + '\\t' + k_n_text\n",
    "\n",
    "        # fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": q_text,\n",
    "                    \"q_n_text\": q_n_text,\n",
    "                    \"k_text\": k_text,\n",
    "                    \"k_n_text\": k_n_text,\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17799 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17799/17799 [00:01<00:00, 11469.07it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "with open(os.path.join(dataset_path, \"test.text.jsonl\"), \"w\") as fout:\n",
    "    for q, k in tqdm(test_pairs):\n",
    "\n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        # random.shuffle(q_n_pool)\n",
    "        # random.shuffle(k_n_pool)\n",
    "\n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num - len(q_n_pool))\n",
    "\n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num - len(k_n_pool))\n",
    "\n",
    "        # prepare for writing file\n",
    "        q_text = data[q]\n",
    "        # q_n_text = '\\*\\*'.join([text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples])\n",
    "        q_n_text = [data[q_n] if q_n != -1 else \"\" for q_n in q_samples]\n",
    "\n",
    "        k_text = data[k]\n",
    "        # k_n_text = '\\*\\*'.join([text_process(data[k_n]['title']) if k_n != -1 else '' for k_n in k_samples])\n",
    "        k_n_text = [data[k_n] if k_n != -1 else \"\" for k_n in k_samples]\n",
    "\n",
    "        # q_line = q_text + '\\t' + q_n_text\n",
    "        # k_line = k_text + '\\t' + k_n_text\n",
    "\n",
    "        # fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": q_text,\n",
    "                    \"q_n_text\": q_n_text,\n",
    "                    \"k_text\": k_text,\n",
    "                    \"k_n_text\": k_n_text,\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# save side files\n",
    "pickle.dump(\n",
    "    [sample_neighbor_num],\n",
    "    open(\n",
    "        os.path.join(dataset_path, \"neighbor_sampling.pkl\"),\n",
    "        \"wb\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# %%\n",
    "# save neighbor file\n",
    "pickle.dump(\n",
    "    train_neighbor,\n",
    "    open(\n",
    "        os.path.join(dataset_path, \"neighbor/train_neighbor.pkl\"),\n",
    "        \"wb\",\n",
    "    ),\n",
    ")\n",
    "pickle.dump(\n",
    "    val_neighbor,\n",
    "    open(\n",
    "        os.path.join(dataset_path, \"neighbor/val_neighbor.pkl\"),\n",
    "        \"wb\",\n",
    "    ),\n",
    ")\n",
    "pickle.dump(\n",
    "    test_neighbor,\n",
    "    open(\n",
    "        os.path.join(dataset_path, \"neighbor/test_neighbor.pkl\"),\n",
    "        \"wb\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [00:00<00:00, 30136.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# save node labels\n",
    "random.seed(0)\n",
    "\n",
    "with open(\n",
    "    os.path.join(dataset_path, \"nc/node_classification.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout:\n",
    "    for q in tqdm(ref_paper):\n",
    "\n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        # random.shuffle(q_n_pool)\n",
    "\n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num - len(q_n_pool))\n",
    "\n",
    "        # prepare for writing file\n",
    "        q_text = data[q]\n",
    "        # q_n_text = '\\*\\*'.join([text_process(data[q_n]['title']) if q_n != -1 else '' for q_n in q_samples])\n",
    "        q_n_text = [data[q_n] if q_n != -1 else \"\" for q_n in q_samples]\n",
    "\n",
    "        label_names_list = np.array(datasets.label_name[datasets.y[q]]).tolist()\n",
    "        label_ids_list = str(np.array(datasets.y[q]))\n",
    "        # print(label_ids_list)\n",
    "        fout.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": q_text,\n",
    "                    \"q_n_text\": q_n_text,\n",
    "                    \"labels\": label_ids_list,\n",
    "                    \"label_names\": label_names_list,\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [00:00<00:00, 439409.64it/s]\n",
      "100%|██████████| 19717/19717 [00:00<00:00, 117487.99it/s]\n",
      "100%|██████████| 3943/3943 [00:00<00:00, 119904.16it/s]\n",
      "100%|██████████| 3944/3944 [00:00<00:00, 122483.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# generate self constrastive pretraining\n",
    "\n",
    "corpus_list = []\n",
    "\n",
    "with open(\n",
    "    os.path.join(dataset_path, \"corpus.txt\"),\n",
    ") as f:\n",
    "    readin = f.readlines()\n",
    "    for index, line in enumerate(tqdm(readin)):\n",
    "        tmp = line.strip().split(\"\\t\")\n",
    "        if len(tmp) > 1:\n",
    "            corpus_list.append(tmp[1])\n",
    "        else:\n",
    "            print(f\"跳过无效行 {index + 1}: {line}\")\n",
    "with open(\n",
    "    os.path.join(dataset_path, \"self-train/train.text.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout:\n",
    "    for dd in tqdm(corpus_list):\n",
    "        fout.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": dd,\n",
    "                    \"q_n_text\": [\"\"],\n",
    "                    \"k_text\": dd,\n",
    "                    \"k_n_text\": [\"\"],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "with open(\n",
    "    os.path.join(dataset_path, \"self-train/val.text.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout:\n",
    "    for dd in tqdm(corpus_list[: int(0.2 * len(corpus_list))]):\n",
    "        fout.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": dd,\n",
    "                    \"q_n_text\": [\"\"],\n",
    "                    \"k_text\": dd,\n",
    "                    \"k_n_text\": [\"\"],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "with open(\n",
    "    os.path.join(dataset_path, \"self-train/test.text.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout:\n",
    "    for dd in tqdm(corpus_list[int(0.8 * len(corpus_list)) :]):\n",
    "        fout.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": dd,\n",
    "                    \"q_n_text\": [\"\"],\n",
    "                    \"k_text\": dd,\n",
    "                    \"k_n_text\": [\"\"],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19717it [00:00, 5712052.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Generate node classification data for retrieval and reranking\n",
    "\n",
    "# %%\n",
    "# write labels into documents.json\n",
    "\n",
    "labels_dict = []\n",
    "label_name2id_dict = {\n",
    "    i: text\n",
    "    for i, text in tqdm(enumerate([datasets.label_name[index] for index in datasets.y]))\n",
    "}\n",
    "# for lid in label_name_dict:\n",
    "for lname in label_name2id_dict:\n",
    "    if lname != \"null\":\n",
    "        labels_dict.append({\"id\": lname, \"contents\": label_name2id_dict[lname]})\n",
    "json.dump(\n",
    "    labels_dict,\n",
    "    open(\n",
    "        os.path.join(dataset_path, \"nc/documents.json\"),\n",
    "        \"w\",\n",
    "    ),\n",
    "    indent=4,\n",
    ")\n",
    "\n",
    "with open(\n",
    "    os.path.join(dataset_path, \"nc/documents.txt\"),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    for i, text in label_name2id_dict.items():\n",
    "        f.write(f\"{i}\\t{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [00:00<00:00, 94904.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# generate node query file & ground truth file\n",
    "\n",
    "docid = 0\n",
    "\n",
    "with open(\n",
    "    os.path.join(dataset_path, \"nc/node_classification.jsonl\"),\n",
    ") as f, open(\n",
    "    os.path.join(dataset_path, \"nc/node_text.tsv\"),\n",
    "    \"w\",\n",
    ") as fout1, open(os.path.join(dataset_path, \"nc/truth.trec\"), \"w\") as fout2:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = json.loads(line)\n",
    "        fout1.write(str(docid) + \"\\t\" + tmp[\"q_text\"] + \"\\n\")\n",
    "        for label in tmp[\"labels\"]:\n",
    "            fout2.write(\n",
    "                str(docid) + \" \" + str(0) + \" \" +\n",
    "                str(label) + \" \" + str(1) + \"\\n\"\n",
    "            )\n",
    "        docid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15773/15773 [00:04<00:00, 3231.89it/s]\n",
      "100%|██████████| 1972/1972 [00:00<00:00, 2983.57it/s]\n",
      "100%|██████████| 1972/1972 [00:00<00:00, 40127.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# generate node query file & ground truth file\n",
    "\n",
    "docid = 0\n",
    "\n",
    "with open(\n",
    "    os.path.join(dataset_path, \"nc/node_classification.jsonl\"),\n",
    ") as f, open(\n",
    "    os.path.join(dataset_path, \"nc/train.text.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout1, open(\n",
    "    os.path.join(dataset_path, \"nc/val.text.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout2, open(\n",
    "    os.path.join(dataset_path, \"nc/test.truth.trec\"),\n",
    "    \"w\",\n",
    ") as fout3, open(\n",
    "    os.path.join(dataset_path, \"nc/test.node.text.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout4:\n",
    "    readin = f.readlines()\n",
    "    total_len = len(readin)\n",
    "    for line in tqdm(readin[: int(0.8 * total_len)]):\n",
    "        tmp = json.loads(line)\n",
    "        for label_name in tmp[\"label_names\"]:\n",
    "            fout1.write(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        \"q_text\": tmp[\"q_text\"],\n",
    "                        \"q_n_text\": tmp[\"q_n_text\"],\n",
    "                        \"k_text\": label_name,\n",
    "                        \"k_n_text\": [\"\"],\n",
    "                    }\n",
    "                )\n",
    "                + \"\\n\"\n",
    "            )\n",
    "        docid += 1\n",
    "\n",
    "    for line in tqdm(readin[int(0.8 * total_len) : int(0.9 * total_len)]):\n",
    "        tmp = json.loads(line)\n",
    "        for label_name in tmp[\"label_names\"]:\n",
    "            fout2.write(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        \"q_text\": tmp[\"q_text\"],\n",
    "                        \"q_n_text\": tmp[\"q_n_text\"],\n",
    "                        \"k_text\": label_name,\n",
    "                        \"k_n_text\": [\"\"],\n",
    "                    }\n",
    "                )\n",
    "                + \"\\n\"\n",
    "            )\n",
    "        docid += 1\n",
    "\n",
    "    for line in tqdm(readin[int(0.9 * total_len) :]):\n",
    "        tmp = json.loads(line)\n",
    "        # fout4.write(str(docid) + '\\t' + tmp['q_text'] + '\\n')\n",
    "        fout4.write(\n",
    "            json.dumps(\n",
    "                {\"id\": str(docid), \"text\": tmp[\"q_text\"], \"n_text\": tmp[\"q_n_text\"]}\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )\n",
    "        for label in tmp[\"labels\"]:\n",
    "            fout3.write(\n",
    "                str(docid) + \" \" + str(0) + \" \" + str(label) + \" \" + str(1) + \"\\n\"\n",
    "            )\n",
    "        docid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19717it [00:00, 6176183.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 19717/19717 [00:00<00:00, 2555596.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique labels:3;{'0': 'Experimental', '1': 'Diabetes Mellitus Type 1', '2': 'Diabetes Mellitus Type 2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [00:00<00:00, 59411.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "label_name2id_dict = {\n",
    "    i: text\n",
    "    for i, text in tqdm(enumerate([datasets.label_name[index] for index in datasets.y]))\n",
    "}\n",
    "label_name2id_dict\n",
    "with open(os.path.join(dataset_path, \"label.txt\"), \"w\") as f:\n",
    "    for id, label in label_name2id_dict.items():\n",
    "        f.write(f\"{id}\\t{label}\\n\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Generate Coarse-grained Classification Data\n",
    "\n",
    "# %%\n",
    "# 保存标签名到txt文件\n",
    "with open(os.path.join(dataset_path, \"labels.txt\"), \"w\") as f:\n",
    "    for idx in datasets.y:\n",
    "        label = datasets.label_name[idx]\n",
    "        f.write(f\"{idx}\\t{label}\\n\")\n",
    "\n",
    "# %%\n",
    "# # read label name dict\n",
    "coarse_label_id2name = {}\n",
    "coarse_label_id2idx = {}\n",
    "\n",
    "with open(os.path.join(dataset_path, \"labels.txt\")) as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = line.strip().split(\"\\t\")\n",
    "        # if tmp[2] == '1':\n",
    "        coarse_label_id2name[tmp[0]] = tmp[1]\n",
    "        coarse_label_id2idx[tmp[0]] = len(coarse_label_id2idx)\n",
    "        # print(coarse_label_id2idx[tmp[0]])\n",
    "\n",
    "print(\n",
    "    f\"Num of unique labels:{len(coarse_label_id2name)};{coarse_label_id2name}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Take care here, you need to generate data for 8 & 16 respectively.\n",
    "\n",
    "# %%\n",
    "# generate train/val/test file\n",
    "# filter out and only use node which has single label\n",
    "\n",
    "ktrain = (\n",
    "    8  # train sample threshold, how many training samples do we have for each class\n",
    ")\n",
    "kdev = 8  # dev sample threshold, how many dev samples do we have for each class\n",
    "label_samples = defaultdict(list)\n",
    "train_mask = datasets.train_mask\n",
    "val_mask = datasets.val_mask\n",
    "test_mask = datasets.test_mask\n",
    "with open(\n",
    "    os.path.join(\n",
    "        dataset_path,\n",
    "        \"nc/node_classification.jsonl\",\n",
    "    ),\n",
    ") as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = json.loads(line)\n",
    "        inter_label = list(tmp[\"labels\"])\n",
    "        # print(inter_label)\n",
    "        # if len(inter_label) == 1:\n",
    "        label_samples[inter_label[0]].append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# with open(f'/home/yuhanli/GLBench/models/alignment/Patton/data/{dataset}/nc/node_classification.jsonl') as f:\n",
    "#     data = f.readlines()\n",
    "# print(data[0])\n",
    "\n",
    "# %%\n",
    "# save\n",
    "if len(train_mask) == 10:\n",
    "    train_mask = train_mask[0]\n",
    "    val_mask = val_mask[0]\n",
    "    test_mask = test_mask[0]\n",
    "\n",
    "if not os.path.exists(\n",
    "    os.path.join(dataset_path, f\"nc-coarse/{str(ktrain)}_{str(kdev)}\")\n",
    "):\n",
    "    os.mkdir(os.path.join(dataset_path, f\"nc-coarse/{str(ktrain)}_{str(kdev)}\"))\n",
    "train_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "with open(\n",
    "    os.path.join(dataset_path, f\"nc/node_classification.jsonl\"),\n",
    ") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        json_obj = json.loads(line)\n",
    "        if train_mask[idx]:\n",
    "            train_data.append(json_obj)\n",
    "        elif val_mask[idx]:\n",
    "            dev_data.append(json_obj)\n",
    "        elif test_mask[idx]:\n",
    "            test_data.append(json_obj)\n",
    "\n",
    "with open(\n",
    "    os.path.join(dataset_path, f\"nc-coarse/{str(ktrain)}_{str(kdev)}/train.text.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout1, open(\n",
    "    os.path.join(dataset_path, f\"nc-coarse/{str(ktrain)}_{str(kdev)}/val.text.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout2, open(\n",
    "    os.path.join(dataset_path, f\"nc-coarse/{str(ktrain)}_{str(kdev)}/test.text.jsonl\"),\n",
    "    \"w\",\n",
    ") as fout3:\n",
    "\n",
    "    # 写入训练数据\n",
    "    for idx, d in enumerate(train_data):\n",
    "        fout1.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": d[\"q_text\"],\n",
    "                    \"q_n_text\": d[\"q_n_text\"],\n",
    "                    \"label\": int(d[\"labels\"]),\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "    # 写入验证数据()\n",
    "    for idx, d in enumerate(dev_data):\n",
    "        fout2.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": d[\"q_text\"],\n",
    "                    \"q_n_text\": d[\"q_n_text\"],\n",
    "                    \"label\": int(d[\"labels\"]),\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "    # # 写入测试数据\n",
    "    for idx, d in enumerate(test_data):\n",
    "        fout3.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"q_text\": d[\"q_text\"],\n",
    "                    \"q_n_text\": d[\"q_n_text\"],\n",
    "                    \"label\": int(d[\"labels\"]),\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "pickle.dump(\n",
    "    coarse_label_id2idx,\n",
    "    open(\n",
    "        os.path.join(dataset_path, f\"nc-coarse/coarse_label_id2idx.pkl\"),\n",
    "        \"wb\",\n",
    "    ),\n",
    ")\n",
    "pickle.dump(\n",
    "    [ktrain, kdev],\n",
    "    open(\n",
    "        os.path.join(dataset_path, f\"nc-coarse/threshold.pkl\"),\n",
    "        \"wb\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# %%\n",
    "datasets.label_name\n",
    "\n",
    "# %%\n",
    "ktrain = (\n",
    "    8  # train sample threshold, how many training samples do we have for each class\n",
    ")\n",
    "kdev = 8  #\n",
    "with open(\n",
    "    os.path.join(dataset_path, f\"nc-coarse/{str(ktrain)}_{str(kdev)}/label_name.txt\"),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    for label in datasets.label_name:\n",
    "        f.write(label + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
